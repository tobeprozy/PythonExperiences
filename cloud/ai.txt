0、引子

本文内容主要摘自百度百科、维基百科。

人工智能是计算机学科的一个分支，二十世纪七十年代以来被称为世界三大尖端技术之一（空间技术、能源技术、人工智能），也被认为是二十一世纪三大尖端技术（基因工程、纳米科学、人工智能）之一，近三十年来它获得了迅速的发展，在机器翻译，智能控制，专家系统，机器人学，语言和图像理解等众多学科领域都获得了广泛应用并取得了丰硕的成果。

一、人工智能定义

早期人们对人工智能的理解不同，一些人认为人工智能是通过非生物系统实现的任何智能形式的同义词，智能的实现方式与人类智能的实现是否相同是无关紧要的；而另一些人认为，人工智能系统必须能够模仿人类智能。随着人工智能技术的发展和应用，人工智能的定义更倾向于第一种说法，人工智能分为“强人工智能”和“弱人工智能”。强人工智能认为有可能制造出真正能推理和解决问题的智能机器，这样的机器是有知觉的，有自我意识的。强人工智能可以有两类：一类是类人的人工智能，即机器的思考和推理就像人的思维一样；一类是非类人的人工智能，即机器产生了和人完全不一样的知觉和意识，使用和人完全不一样的推理方式。弱人工智能认为不可能制造出能真正地推理和解决问题的智能机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能，也不会有自主意识。

约翰·麦卡锡于1955年的定义是[4]“制造智能机器的科学与工程”[5]。安德里亚斯•卡普兰（Andreas Kaplan）和迈克尔•海恩莱因（Michael Haenlein）将人工智能定义为“系统正确解释外部数据，从这些数据中学习，并利用这些知识通过灵活适应实现特定目标和任务的能力”。维基百科上对人工智能的定义是：人工智能是指由人制造出来的机器所表现出来的智能。百度百科上对人工知能的定义是：人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。维基百科上的定义简单明了，百度百科的定义更正式一些，总体来说都倾向于给人工智能一个更广义的定义，即人工智能是模仿人类但不局限人类的一切人工制造的智能形式，包含强人工智能、弱人工智能的一切形态。

二、人工智能的发展简史

1、人工智能的诞生：1943~1956

       20世纪40年代和50年代，来自不同领域（数学，心理学，工程学，经济学和政治学）的一批科学家开始探讨制造人工大脑的可能性。1956年，人工智能被确立为一门学科。

1）控制论与早期神经网络

最初的人工智能研究是30年代末到50年代初的一系列科学进展交汇的产物。神经学研究发现大脑是由神经元组成的电子网络，其激励电平只存在“有”和“无”两种状态，不存在中间状态。维纳的控制论描述了电子网络的控制和稳定性。克劳德·香农提出的信息论则描述了数字信号（即高低电平代表的二进制信号）。图灵的计算理论证明数字信号足以描述任何形式的计算。这些密切相关的想法暗示了构建电子大脑的可能性。

这一阶段的工作包括一些机器人的研发，例如W. Grey Walter的“乌龟（turtles）”，还有“约翰霍普金斯兽”（Johns Hopkins Beast）。这些机器并未使用计算机、数字电路和符号推理；控制它们的是纯粹的模拟电路。

Walter Pitts和Warren McCulloch分析了理想化的人工神经元网络，并且指出了它们进行简单逻辑运算的机制。他们是最早描述所谓“神经网络”的学者。1951年，他们的学生，马文·明斯基，与Dean Edmonds一道建造了第一台神经网络机，称为SNARC。

2）游戏AI

1951年，Christopher Strachey使用曼彻斯特大学的Ferranti Mark 1机器写出了一个西洋跳棋（checkers）程序，Dietrich Prinz则写出了一个国际象棋程序，亚瑟·山谬尔（Arthur Samuel）在五十年代中期和六十年代初开发的国际象棋程序的棋力已经可以挑战具有相当水平的业余爱好者。

3）图灵测试

1950年，图灵发表了一篇划时代的论文，文中预言了创造出具有真正智能的机器的可能性。由于注意到“智能”这一概念难以确切定义，他提出了著名的图灵测试：如果一台机器能够与人类展开对话（通过电传设备）而不能被辨别出其机器身份，那么称这台机器具有智能。这一简化使得图灵能够令人信服地说明“思考的机器”是可能的，论文中还回答了对这一假说的各种常见质疑。图灵测试是人工智能哲学方面第一个严肃的提案。

4）符号推理与“逻辑理论家”

50年代中期，随着数字计算机的兴起，一些科学家直觉地感到可以进行数字操作的机器也应当可以进行符号操作，而符号操作可能是人类思维的本质。这是创造智能机器的一条新路。

1955年，艾伦·纽厄尔和后来荣获诺贝尔奖的赫伯特·西蒙在J. C. Shaw的协助下开发了“逻辑理论家（Logic Theorist）”。这个程序能够证明《数学原理》中前52个定理中的38个，其中某些证明比原著更加新颖和精巧。Simon认为他们已经“解决了神秘的心/身问题，解释了物质构成的系统如何获得心灵的性质。这一断言的哲学立场后来被John Searle称为“强人工智能”，即机器可以像人一样具有思想。

5）1956年达特茅斯会议：AI诞生

1956年达特矛斯会议的组织者是马文·明斯基，约翰·麦卡锡和另两位资深科学家克劳德·香农以及内森·罗彻斯特（Nathan Rochester），后者来自IBM。会议提出的断言之一是“学习或者智能的任何其他特性的每一个方面都应能被精确地加以描述，使得机器可以对其进行模拟。会上纽厄尔和西蒙讨论了“逻辑理论家”，而麦卡锡则说服与会者接受“人工智能”一词作为本领域的名称。1956年达特矛斯会议上AI的名称和任务得以确定，同时出现了最初的成就和最早的一批研究者，因此这一事件被广泛承认为AI诞生的标志。

2、黄金年代：1956~1974

达特茅斯会议之后的数年是大发现的时代，对许多人而言，这一阶段开发出的程序堪称神奇：[4计算机可以解决代数应用题，证明几何定理，学习和使用英语。当时大多数人几乎无法相信机器能够如此“智能”。DARPA（国防高等研究计划署）等政府机构向这一新兴领域投入了大笔资金。研究者们在私下的交流和公开发表的论文中表达出相当乐观的情绪，认为具有完全智能的机器将在二十年内出现，曾做出以下预言：

1958年，艾伦·纽厄尔和赫伯特·西蒙：“十年之内，数字计算机将成为国际象棋世界冠军。” “十年之内，数字计算机将发现并证明一个重要的数学定理。
1965年，赫伯特·西蒙：“二十年内，机器将能完成人能做到的一切工作。”
1967年，马文·闵斯基：“一代之内……创造‘人工智能’的问题将获得实质上的解决。”
1970年，马文·闵斯基：“在三到八年的时间里我们将得到一台具有人类平均智能的机器。”
从50年代后期到60年代涌现了大批成功的AI程序和新的研究方向，下面列举最具影响的几个。

1）搜索式推理

许多AI程序使用相同的基本算法。为实现一个目标（例如赢得游戏或证明定理），它们一步步地前进，就像在迷宫中寻找出路一般；如果遇到了死胡同则进行回溯。这就是“搜索式推理”。这一思想遇到的主要困难是，在很多问题中，“迷宫”里可能的线路总数是一个天文数字（所谓“指数爆炸”）。研究者使用启发式算法去掉那些不太可能导出正确答案的支路，从而缩小搜索范围。艾伦·纽厄尔和赫伯特·西蒙试图通过其“通用解题器（General Problem Solver）”程序，将这一算法推广到一般情形。另一些基于搜索算法证明几何与代数问题的程序也给人们留下了深刻印象，例如赫伯特·吉宁特（Herbert Gelernter）的几何定理证明机（1958）和马文·李·闵斯基的学生James Slagle开发的SAINT（1961）。还有一些程序通过搜索目标和子目标作出决策，如斯坦福大学为控制机器人Shakey而开发的STRIPS系统。

2）自然语言处理

AI研究的一个重要目标是使计算机能够通过自然语言进行交流。如果用节点表示语义概念（例如“房子”，“门”），用节点间的连线表示语义关系（例如“有 -- 一个”），就可以构造出“语义网（semantic net）”。第一个使用语义网的AI程序由Ross Quillian开发；而最为成功（也是最有争议）的一个则是Roger Schank的“概念关联（Conceptual Dependency）”。 Joseph Weizenbaum的ELIZA是第一个聊天机器人，可能也是最有趣的会说英语的程序。与ELIZA“聊天”的用户有时会误以为自己是在和人类，而不是和一个程序交谈；但是实际上ELIZA根本不知道自己在说什么，它只是按固定套路作答，或者用符合语法的方式将问题复述一遍。

3）微世界

60年代后期，麻省理工大学AI实验室的马文·闵斯基和西摩尔·派普特建议AI研究者们专注于被称为“微世界”的简单场景。他们指出在成熟的学科中往往使用简化模型帮助基本原则的理解，例如物理学中的光滑平面和完美刚体。许多这类研究的场景是“积木世界”，其中包括一个平面，上面摆放着一些不同形状，尺寸和颜色的积木。在这一指导思想下，杰拉德·杰伊·萨斯曼（研究组长），Adolfo Guzman，大卫·瓦尔兹（David Waltz，“约束传播（constraint propagation）”的提出者），特别是Patrick Winston等人在机器视觉领域作出了创造性贡献。同时，Minsky和Papert制作了一个会搭积木的机器臂，从而将“积木世界”变为现实。微世界程序的最高成就是Terry Winograd的SHRDLU，它能用普通的英语句子与人交流，还能作出决策并执行操作。

3、第一次AI低谷：1974~1980

到了70年代，AI开始遭遇批评，随之而来的还有资金上的困难。AI研究者们对其课题的难度未能作出正确判断：此前的过于乐观使人们期望过高，当承诺无法兑现时，对AI的资助就缩减或取消了。同时，由于马文·闵斯基对感知器的激烈批评，联结主义（即神经网络）销声匿迹了十年。即使是最杰出的AI程序也只能解决它们尝试解决的问题中最简单的一部分，也就是说所有的AI程序都只是“玩具”。AI研究者们遭遇了无法克服的基础性障碍。

计算机的运算能力。当时的计算机有限的内存和处理速度不足以解决任何实际的AI问题。例如，罗斯·奎利恩（Ross Quillian）在自然语言方面的研究结果只能用一个含二十个单词的词汇表进行演示，因为内存只能容纳这么多。1976年，汉斯·莫拉维克指出，计算机离智能的要求还差上百万倍。他做了个类比：人工智能需要强大的计算能力，就像飞机需要大功率动力一样，低于一个门限时是无法实现的；但是随着能力的提升，问题逐渐会变得简单。
计算复杂性和指数爆炸。1972年理查德·卡普根据史提芬·古克于1971年提出的Cook-Levin理论证明，许多问题只可能在指数时间内获解（即，计算时间与输入规模的幂成正比）。除了那些最简单的情况，这些问题的解决需要近乎无限长的时间。这就意味着AI中的许多玩具程序恐怕永远也不会发展为实用的系统。
常识与推理。许多重要的AI应用，例如机器视觉和自然语言，都需要大量对世界的认识信息。程序应该知道它在看什么，或者在说些什么，这要求程序对这个世界具有至少儿童水平的认识。研究者们很快发现这个要求太高了：1970年没人能够做出如此巨大的数据库，也没人知道一个程序怎样才能学到如此丰富的信息。
莫拉维克悖论。证明定理和解决几何问题对计算机而言相对容易，而一些看似简单的任务，如人脸识别或穿过屋子，实现起来却极端困难。这也是70年代中期机器视觉和机器人方面进展缓慢的原因。
框架和资格问题。采取逻辑观点的AI研究者们（例如John McCarthy）发现，如果不对逻辑的结构进行调整，他们就无法对常见的涉及自动规划（planning or default reasoning）的推理进行表达。为解决这一问题，他们发展了新逻辑学（如非单调逻辑（non-monotonic logics）和模态逻辑（modal logics））。
由于缺乏进展，对AI提供资助的机构（如英国政府，DARPA和NRC）对无方向的AI研究逐渐停止了资助。
一些哲学家强烈反对AI研究者的主张。其中最早的一个是John Lucas，他认为哥德尔不完备定理已经证明形式系统（例如计算机程序）不可能判断某些陈述的真理性，但是人类可以。修伯特·德雷福斯（Hubert Dreyfus）讽刺六十年代AI界那些未实现的预言，并且批评AI的基础假设，认为人类推理实际上仅涉及少量“符号处理”，而大多是具体的，直觉的，下意识的“窍门（know how）”。
这期间，主要领域研究情况如下。

1）感知器与联结主义遭到冷落

感知器是神经网络的一种形式，由Frank Rosenblatt于1958年提出。与多数AI研究者一样，他对这一发明的潜力非常乐观，预言说“感知器最终将能够学习，作出决策和翻译语言”。整个六十年代里这一方向的研究工作都很活跃。1969年Minsky和Papert出版了著作《感知器》，书中暗示感知器具有严重局限，而Frank Rosenblatt的预言过于夸张。这本书的影响是破坏性的：联结主义的研究因此停滞了十年。后来新一代研究者使这一领域获得重生，并使其成为人工智能中的重要部分；遗憾的是Rosenblatt没能看到这些，他在《感知器》问世后不久即因游船事故去世。

2）“简约派（the neats）”：逻辑，Prolog语言和专家系统

早在1958年，John McCarthy就提出了名为“纳谏者（Advice Taker）”的一个程序构想，将逻辑学引入了AI研究界。1963年，J. Alan Robinson发现了在计算机上实现推理的简单方法：归结（resolution）与合一（unification）算法。然而，根据60年代末McCarthy和他的学生们的工作，对这一想法的直接实现具有极高的计算复杂度：即使是证明很简单的定理也需要天文数字的步骤。70年代Robert Kowalsky在Edinburgh大学的工作则更具成效：法国学者Alain Colmerauer和Phillipe Roussel在他的合作下开发出成功的逻辑编程语言Prolog。

Dreyfus等人针对逻辑方法的批评观点认为，人类在解决问题时并没有使用逻辑运算。心理学家Peter Wason，Eleanor Rosch，阿摩司·特沃斯基，Daniel Kahneman等人的实验证明了这一点。McCarthy则回应说，人类怎么思考是无关紧要的：真正想要的是解题机器，而不是模仿人类进行思考的机器。

3）芜杂派（the scruffies）”：框架和脚本

对McCarthy的做法持批评意见的还有他在MIT的同行们。马文·闵斯基，Seymour Papert和Roger Schank等试图让机器像人一样思考，使之能够解决“理解故事”和“目标识别”一类问题。为了使用“椅子”，“饭店”之类最基本的概念，他们需要让机器像人一样作出一些非逻辑的假设。不幸的是，这些不精确的概念难以用逻辑进行表达。Gerald Sussman注意到，“使用精确的语言描述本质上不精确的概念，并不能使它们变得精确起来”。Schank用“芜杂（scruffy）”一词描述他们这一“反逻辑”的方法，与McCarthy，Kowalski，Feigenbaum，Newell和Simon等人的“简约（neat）”方案相对。

在1975年的一篇开创性论文中，Minsky注意到与他共事的“芜杂派”研究者在使用同一类型的工具，即用一个框架囊括所有相关的常识性假设。例如，当我们使用“鸟”这一概念时，脑中会立即浮现出一系列相关事实，如会飞，吃虫子，等等。我们知道这些假设并不一定正确，使用这些事实的推理也未必符合逻辑，但是这一系列假设组成的结构正是我们所想和所说的一部分。他把这个结构称为“框架（frames）”。Schank使用了“框架”的一个变种，他称之为“脚本（scripts）”，基于这一想法他使程序能够回答关于一篇英语短文的提问。多年之后的面向对象编程采纳了AI“框架”研究中的“继承（inheritance）”概念。

4、繁荣：1980 - 1987

在80年代，一类名为“专家系统”的AI程序开始为全世界的公司所采纳，而“知识处理”成为了主流AI研究的焦点。日本政府在同一年代积极投资AI以促进其第五代计算机工程。80年代早期另一个令人振奋的事件是John Hopfield和David Rumelhart使联结主义重获新生。AI再一次获得了成功。

1）专家系统获得赏识

专家系统是一种程序，能够依据一组从专门知识中推演出的逻辑规则在某一特定领域回答或解决问题。最早的示例由Edward Feigenbaum和他的学生们开发。1965年起设计的Dendral能够根据分光计读数分辨混合物。1972年设计的MYCIN能够诊断血液传染病。它们展示了这一方法的威力。

专家系统仅限于一个很小的知识领域，从而避免了常识问题；其简单的设计又使它能够较为容易地编程实现或修改。总之，实践证明了这类程序的实用性。直到现在AI才开始变得实用起来。

1980年CMU为DEC（Digital Equipment Corporation，数字设备公司）设计了一个名为XCON的专家系统，这是一个巨大的成功。在1986年之前，它每年为公司省下四千万美元。全世界的公司都开始研发和应用专家系统，到1985年它们已在AI上投入十亿美元以上，大部分用于公司内设的AI部门。为之提供支持的产业应运而生，其中包括Symbolics，Lisp Machines等硬件公司和IntelliCorp，Aion等软件公司。

2）知识革命

专家系统的能力来自于它们存储的专业知识。这是70年代以来AI研究的一个新方向。 Pamela McCorduck在书中写道，“不情愿的AI研究者们开始怀疑，因为它违背了科学研究中对最简化的追求。智能可能需要建立在对分门别类的大量知识的多种处理方法之上。” “70年代的教训是智能行为与知识处理关系非常密切。有时还需要在特定任务领域非常细致的知识。” 知识库系统和知识工程成为了80年代AI研究的主要方向。

第一个试图解决常识问题的程序Cyc也在80年代出现，其方法是建立一个容纳一个普通人知道的所有常识的巨型数据库。发起和领导这一项目的Douglas Lenat认为别无捷径，让机器理解人类概念的唯一方法是一个一个地教会它们。这一工程几十年也没有完成。

3）重获拨款：第五代工程

1981年，日本经济产业省拨款八亿五千万美元支持第五代计算机项目。其目标是造出能够与人对话，翻译语言，解释图像，并且像人一样推理的机器。令“芜杂派”不满的是，他们选用Prolog作为该项目的主要编程语言。

其他国家纷纷作出响应。英国开始了耗资三亿五千万英镑的Alvey工程。美国一个企业协会组织了MCC（Microelectronics and Computer Technology Corporation，微电子与计算机技术集团），向AI和信息技术的大规模项目提供资助。DARPA也行动起来，组织了战略计算促进会（Strategic Computing Initiative），其1988年向AI的投资是1984年的三倍。

4）联结主义的重生

1982年，物理学家John Hopfield证明一种新型的神经网络（现被称为“Hopfield网络”）能够用一种全新的方式学习和处理信息。大约在同时（早于Paul Werbos），David Rumelhart推广了反向传播算法，一种神经网络训练方法。这些发现使1970年以来一直遭人遗弃的联结主义重获新生。

1986年由Rumelhart和心理学家James McClelland主编的两卷本论文集“分布式并行处理”问世，这一新领域从此得到了统一和促进。90年代神经网络获得了商业上的成功，它们被应用于光字符识别和语音识别软件。

5）第二次AI低谷：1987 - 1993

80年代中商业机构对AI的追捧与冷落符合经济泡沫的经典模式，泡沫的破裂也在政府机构和投资者对AI的观察之中。尽管遇到各种批评，这一领域仍在不断前进。来自机器人学这一相关研究领域的Rodney Brooks和Hans Moravec提出了一种全新的人工智能方案。

“AI之冬”一词由经历过1974年经费削减的研究者们创造出来。他们注意到了对专家系统的狂热追捧，预计不久后人们将转向失望。事实被他们不幸言中：从80年代末到90年代初，AI遭遇了一系列财政问题。变天的最早征兆是1987年AI硬件市场需求的突然下跌。Apple和IBM生产的台式机性能不断提升，到1987年时其性能已经超过了Symbolics和其他厂家生产的昂贵的Lisp机。老产品失去了存在的理由：一夜之间这个价值五亿美元的产业土崩瓦解。XCON等最初大获成功的专家系统维护费用居高不下。它们难以升级，难以使用，脆弱（当输入异常时会出现莫名其妙的错误），成了以前已经暴露的各种各样的问题的牺牲品。专家系统的实用性仅仅局限于某些特定情景。

到了80年代晚期，战略计算促进会大幅削减对AI的资助。DARPA的新任领导认为AI并非“下一个浪潮”，拨款将倾向于那些看起来更容易出成果的项目。直到1991年，“第五代工程”并没有实现，事实上其中一些目标，比如“与人展开交谈”，直到2010年也没有实现。与其他AI项目一样，期望比真正可能实现的要高得多。

80年代后期，一些研究者根据机器人学的成就提出了一种全新的人工智能方案。他们相信，为了获得真正的智能，机器必须具有躯体 - 它需要感知，移动，生存，与这个世界交互。他们认为这些感知运动技能对于常识推理等高层次技能是至关重要的，而抽象推理不过是人类最不重要，也最无趣的技能（参见Moravec悖论）。他们号召“自底向上”地创造智能，这一主张复兴了从60年代就沉寂下来的控制论。另一位先驱是在理论神经科学上造诣深厚的David Marr，他于70年代来到MIT指导视觉研究组的工作。他排斥所有符号化方法（不论是McCarthy的逻辑学还是Minsky的框架），认为实现AI需要自底向上地理解视觉的物理机制，而符号处理应在此之后进行。在发表于1990年的论文“大象不玩象棋（Elephants Don't Play Chess）”中，机器人研究者Rodney Brooks提出了“物理符号系统假设”，认为符号是可有可无的，因为“这个世界就是描述它自己最好的模型。它总是最新的。它总是包括了需要研究的所有细节。诀窍在于正确地，足够频繁地感知它。” 在80年代和90年代也有许多认知科学家反对基于符号处理的智能模型，认为身体是推理的必要条件，这一理论被称为“具身的心灵/理性/ 认知（embodied mind/reason/cognition）”论题。

6）1993~现在

现已年过半百的AI终于实现了它最初的一些目标。它已被成功地用在技术产业中，不过有时是在幕后。这些成就有的归功于计算机性能的提升，有的则是在高尚的科学责任感驱使下对特定的课题不断追求而获得的。不过，至少在商业领域里AI的声誉已经不如往昔了。“实现人类水平的智能”这一最初的梦想曾在60年代令全世界的想象力为之着迷，其失败的原因至今仍众说纷纭。各种因素的合力将AI拆分为各自为战的几个子领域，有时候它们甚至会用新名词来掩饰“人工智能”这块被玷污的金字招牌。AI比以往的任何时候都更加谨慎，却也更加成功。

1）里程碑和摩尔定律

1997年5月11日，深蓝成为战胜国际象棋世界冠军卡斯帕罗夫的第一个计算机系统。2005年，Stanford开发的一台机器人在一条沙漠小径上成功地自动行驶了131英里，赢得了DARPA挑战大赛头奖。2009年，蓝脑计划声称已经成功地模拟了部分鼠脑。2011年，IBM 沃森参加《危险边缘》节目，在最后一集打败了人类选手。2016年3月，AlphaGo击败李世乭，成为第一个不让子而击败职业围棋棋士的电脑围棋程式。2017年5月，AlphaGo在中国乌镇围棋峰会的三局比赛中击败当时世界排名第一的中国棋手柯洁。

这些成就的取得并不是因为范式上的革命。它们仍然是工程技术的复杂应用，但是计算机性能已经今非昔比了。事实上，深蓝计算机比克里斯多福·斯特雷奇（Christopher Strachey）在1951年用来下棋的Ferranti Mark 1快一千万倍。这种剧烈增长可以用摩尔定律描述：计算速度和内存容量每两年翻一番。计算性能上的基础性障碍已被逐渐克服。

2）智能代理

90年代，被称为“智能代理”的新范式被广泛接受。尽管早期研究者提出了模块化的分治策略，但是直到Judea Pearl，Alan Newell等人将一些概念从决策理论和经济学中引入AI之后现代智能代理范式才逐渐形成。当经济学中的“理性代理（rational agent）”与计算机科学中的“对象”或“模块”相结合，“智能代理”范式就完善了。

智能代理是一个系统，它感知周围环境，然后采取措施使成功的几率最大化。最简单的智能代理是解决特定问题的程序。已知的最复杂的智能代理是理性的，会思考的人类。智能代理范式将AI研究定义为“对智能代理的学习”。这是对早期一些定义的推广：它超越了研究人类智能的范畴，涵盖了对所有种类的智能的研究。

这一范式让研究者们通过学习孤立的问题找到可证的并且有用的解答。它为AI各领域乃至经济学，控制论等使用抽象代理概念的领域提供了描述问题和共享解答的一种通用语言。人们希望能找到一种完整的代理架构（像Newell的SOAR（英语：SOAR）那样），允许研究者们应用交互的智能代理建立起通用的智能系统。

3）“简约派”的胜利

越来越多的AI研究者们开始开发和使用复杂的数学工具。人们广泛地认识到，许多AI需要解决的问题已经成为数学，经济学和运筹学领域的研究课题。数学语言的共享不仅使AI可以与其他学科展开更高层次的合作，而且使研究结果更易于评估和证明。AI已成为一门更严格的科学分支。Russell和Norvig（2003）将这些变化视为一场“革命”和“简约派的胜利”。

Judea Pearl发表于1988年的名著将概率论和决策理论引入AI。现已投入应用的新工具包括贝叶斯网络，隐马尔可夫模型，信息论，随机模型和经典优化理论。针对神经网络和进化算法等“计算智能”范式的精确数学描述也被发展出来。

4）幕后的AI

AI研究者们开发的算法开始变为较大的系统的一部分。AI曾经解决了大量的难题，这些解决方案在产业界起到了重要作用。应用了AI技术的有数据挖掘，工业机器人，物流，语音识别，银行业软件，医疗诊断和Google搜索引擎等。

AI领域并未从这些成就之中获得多少益处。AI的许多伟大创新仅被看作计算机科学工具箱中的一件工具。Nick Bostrom解释说，“很多AI的前沿成就已被应用在一般的程序中，不过通常没有被称为AI。这是因为，一旦变得足够有用和普遍，它就不再被称为AI了。”

90年代的许多AI研究者故意用其他一些名字称呼他们的工作，例如信息学，知识系统，认知系统或计算智能。部分原因是他们认为他们的领域与AI存在根本的不同，不过新名字也有利于获取经费。至少在商业领域，导致AI之冬的那些未能兑现的承诺仍然困扰着AI研究，正如New York Times在2005年的一篇报道所说：“计算机科学家和软件工程师们避免使用人工智能一词，因为怕被认为是在说梦话。”

5）HAL 9000在哪里?

1968年亚瑟·克拉克和史丹利·库柏力克创作的《“2001太空漫游”》中设想2001年将会出现达到或超过人类智能的机器。他们创造的这一名为HAL-9000的角色是以科学事实为依据的：当时许多顶极AI研究者相信到2001年这样的机器会出现。

“那么问题是，为什么在2001年我们并未拥有HAL呢?” 马文·闵斯基问道。Minsky认为，问题的答案是绝大多数研究者醉心于钻研神经网络和遗传算法之类商业应用，而忽略了常识推理等核心问题。另一方面，约翰·麦卡锡则归咎于资格问题。雷蒙德·库茨魏尔相信问题在于计算机性能，根据摩尔定律，他预测具有人类智能水平的机器将在2029年出现。杰夫·霍金认为神经网络研究忽略了人类大脑皮质的关键特性，而简单的模型只能用于解决简单的问题。还有许多别的解释，每一个都对应着一个正在进行的研究计划。

6）深度学习，大数据和人工智能：2011至今

进入21世纪，得益于大数据和计算机技术的快速发展，许多先进的机器学习技术成功应用于经济社会中的许多问题。麦肯锡全球研究院在一份题为《大数据：创新、竞争和生产力的下一个前沿领域》的报告中估计，到2009年，美国经济所有行业中具有1000名以上员工的公司都至少平均拥有一个200兆兆字节的存储数据。

到2016年，AI相关产品、硬件、软件等的市场规模已经超过80亿美元，纽约时报评价道AI已经到达了一个热潮。大数据应用也开始逐渐渗透到其他领域，例如生态学模型训练、经济领域中的各种应用、医学研究中的疾病预测及新药研发等。深度学习（特别是深度卷积神经网络和循环网络）更是极大地推动了图像和视频处理、文本分析、语音识别等问题的研究进程。

深度学习是机器学习的一个分支，它通过一个有着很多层处理单元的深层网络对数据中的高级抽象进行建模。根据全局逼近原理（Universal approximation theorem），对于神经网络而言，如果要拟合任意连续函数，深度性并不是必须的，即使一个单层的网络，只要拥有足够多的非线性激活单元，也可以达到拟合目的。但是，目前深度神经网络得到了更多的关注，这主要是源于其结构层次性，能够快速建模更加复杂的情况，同时避免浅层网络可能遭遇的诸多缺点。

然而，深度学习也有自身的缺点。以循环神经网络为例，一个最常见的问题是梯度消失问题（沿着时间序列反向传播过程中，梯度逐渐减小到0附近，造成学习停滞）。为了解决这些问题，很多针对性的模型被提出来，例如LSTM（长短期记忆网络，早在1997年就已经提出，最近随着RNN的大火，又重新进入大众视野）、GRU（门控循环神经单元）等等。

现在，最先进的神经网络结构在某些领域已经能够达到甚至超过人类平均准确率，例如在计算机视觉领域，特别是一些具体的任务上，比如MNIST数据集（一个手写数字识别数据集）、交通信号灯识别等。再如游戏领域，Google的deepmind团队研发的AlaphaGo，在问题搜索复杂度极高的围棋上，已经打遍天下无敌手。

三、人工智能研究内容

人工智能涉及到众多领域，包括数学、统计学、计算机科学、物理学、哲学和认知科学、逻辑学、心理学、控制论、决定论、不确定性原理、社会学、犯罪学等。研究范畴包括自然语言处理（NLP; Natural Language Processing）、知识表现（Knowledge Representation）、智能搜索（Intelligent Search）、推理、规划（Planning）、机器学习（Machine Learning）、增强式学习（Reinforcement Learning）、知识获取、感知问题、模式识别、逻辑程序设计、软计算（Soft Computing）、不精确和不确定的管理、人工生命（Artificial Life）、人工神经网络（Artificial Neural Network）、复杂系统、遗传算法、数据捕捞（Data Mining）、模糊控制等重多方向。人工智能涉及应用领域极广、研究内容极多，暂未找到清晰合理的分类。